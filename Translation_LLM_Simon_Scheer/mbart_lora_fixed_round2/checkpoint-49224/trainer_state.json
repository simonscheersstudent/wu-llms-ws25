{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 49224,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012189547463050435,
      "grad_norm": 2.579293966293335,
      "learning_rate": 7.960000000000001e-05,
      "loss": 1.731,
      "step": 200
    },
    {
      "epoch": 0.02437909492610087,
      "grad_norm": 3.661714792251587,
      "learning_rate": 0.0001596,
      "loss": 1.5304,
      "step": 400
    },
    {
      "epoch": 0.0365686423891513,
      "grad_norm": 2.2492833137512207,
      "learning_rate": 0.0001995936294228717,
      "loss": 1.6477,
      "step": 600
    },
    {
      "epoch": 0.04875818985220174,
      "grad_norm": 3.034590721130371,
      "learning_rate": 0.0001987726787620064,
      "loss": 1.6538,
      "step": 800
    },
    {
      "epoch": 0.06094773731525217,
      "grad_norm": 2.176056385040283,
      "learning_rate": 0.00019795172810114112,
      "loss": 1.5789,
      "step": 1000
    },
    {
      "epoch": 0.0731372847783026,
      "grad_norm": 2.310497760772705,
      "learning_rate": 0.00019713077744027584,
      "loss": 1.5684,
      "step": 1200
    },
    {
      "epoch": 0.08532683224135304,
      "grad_norm": 1.7595524787902832,
      "learning_rate": 0.00019630982677941057,
      "loss": 1.5536,
      "step": 1400
    },
    {
      "epoch": 0.09751637970440348,
      "grad_norm": 1.6975842714309692,
      "learning_rate": 0.0001954888761185453,
      "loss": 1.5151,
      "step": 1600
    },
    {
      "epoch": 0.1097059271674539,
      "grad_norm": 2.7223665714263916,
      "learning_rate": 0.00019466792545768,
      "loss": 1.5111,
      "step": 1800
    },
    {
      "epoch": 0.12189547463050435,
      "grad_norm": 2.8916804790496826,
      "learning_rate": 0.00019384697479681473,
      "loss": 1.4826,
      "step": 2000
    },
    {
      "epoch": 0.13408502209355477,
      "grad_norm": 1.988296389579773,
      "learning_rate": 0.00019302602413594943,
      "loss": 1.4829,
      "step": 2200
    },
    {
      "epoch": 0.1462745695566052,
      "grad_norm": 1.4546502828598022,
      "learning_rate": 0.00019220507347508415,
      "loss": 1.4494,
      "step": 2400
    },
    {
      "epoch": 0.15846411701965565,
      "grad_norm": 2.1083223819732666,
      "learning_rate": 0.00019138412281421887,
      "loss": 1.4221,
      "step": 2600
    },
    {
      "epoch": 0.17065366448270608,
      "grad_norm": 2.466320514678955,
      "learning_rate": 0.0001905631721533536,
      "loss": 1.4533,
      "step": 2800
    },
    {
      "epoch": 0.1828432119457565,
      "grad_norm": 2.2410783767700195,
      "learning_rate": 0.0001897422214924883,
      "loss": 1.4072,
      "step": 3000
    },
    {
      "epoch": 0.19503275940880696,
      "grad_norm": 1.645523190498352,
      "learning_rate": 0.00018892127083162304,
      "loss": 1.4282,
      "step": 3200
    },
    {
      "epoch": 0.2072223068718574,
      "grad_norm": 2.032217025756836,
      "learning_rate": 0.00018810032017075774,
      "loss": 1.4339,
      "step": 3400
    },
    {
      "epoch": 0.2194118543349078,
      "grad_norm": 2.7054836750030518,
      "learning_rate": 0.00018727936950989246,
      "loss": 1.419,
      "step": 3600
    },
    {
      "epoch": 0.23160140179795824,
      "grad_norm": 2.8585147857666016,
      "learning_rate": 0.00018645841884902718,
      "loss": 1.4694,
      "step": 3800
    },
    {
      "epoch": 0.2437909492610087,
      "grad_norm": 2.1394433975219727,
      "learning_rate": 0.0001856374681881619,
      "loss": 1.4344,
      "step": 4000
    },
    {
      "epoch": 0.2559804967240591,
      "grad_norm": 1.6312686204910278,
      "learning_rate": 0.00018481651752729663,
      "loss": 1.3439,
      "step": 4200
    },
    {
      "epoch": 0.26817004418710955,
      "grad_norm": 2.8717041015625,
      "learning_rate": 0.00018399556686643135,
      "loss": 1.365,
      "step": 4400
    },
    {
      "epoch": 0.28035959165016,
      "grad_norm": 2.5612759590148926,
      "learning_rate": 0.00018317461620556607,
      "loss": 1.3306,
      "step": 4600
    },
    {
      "epoch": 0.2925491391132104,
      "grad_norm": 1.6954046487808228,
      "learning_rate": 0.00018235366554470077,
      "loss": 1.3683,
      "step": 4800
    },
    {
      "epoch": 0.30473868657626085,
      "grad_norm": 1.7886232137680054,
      "learning_rate": 0.0001815327148838355,
      "loss": 1.3483,
      "step": 5000
    },
    {
      "epoch": 0.3169282340393113,
      "grad_norm": 2.2110652923583984,
      "learning_rate": 0.0001807117642229702,
      "loss": 1.3451,
      "step": 5200
    },
    {
      "epoch": 0.3291177815023617,
      "grad_norm": 2.5710339546203613,
      "learning_rate": 0.00017989081356210493,
      "loss": 1.3194,
      "step": 5400
    },
    {
      "epoch": 0.34130732896541216,
      "grad_norm": 4.115718364715576,
      "learning_rate": 0.00017906986290123963,
      "loss": 1.4023,
      "step": 5600
    },
    {
      "epoch": 0.3534968764284626,
      "grad_norm": 2.498563289642334,
      "learning_rate": 0.00017824891224037438,
      "loss": 1.3043,
      "step": 5800
    },
    {
      "epoch": 0.365686423891513,
      "grad_norm": 2.625370502471924,
      "learning_rate": 0.00017742796157950907,
      "loss": 1.308,
      "step": 6000
    },
    {
      "epoch": 0.37787597135456347,
      "grad_norm": 2.9475717544555664,
      "learning_rate": 0.0001766070109186438,
      "loss": 1.3114,
      "step": 6200
    },
    {
      "epoch": 0.3900655188176139,
      "grad_norm": 3.487058162689209,
      "learning_rate": 0.00017578606025777852,
      "loss": 1.3252,
      "step": 6400
    },
    {
      "epoch": 0.4022550662806643,
      "grad_norm": 2.3519887924194336,
      "learning_rate": 0.00017496510959691324,
      "loss": 1.3965,
      "step": 6600
    },
    {
      "epoch": 0.4144446137437148,
      "grad_norm": 2.803511619567871,
      "learning_rate": 0.00017414415893604793,
      "loss": 1.2604,
      "step": 6800
    },
    {
      "epoch": 0.4266341612067652,
      "grad_norm": 1.4184141159057617,
      "learning_rate": 0.00017332320827518266,
      "loss": 1.2793,
      "step": 7000
    },
    {
      "epoch": 0.4388237086698156,
      "grad_norm": 3.0815699100494385,
      "learning_rate": 0.0001725022576143174,
      "loss": 1.3711,
      "step": 7200
    },
    {
      "epoch": 0.4510132561328661,
      "grad_norm": 3.0121917724609375,
      "learning_rate": 0.0001716813069534521,
      "loss": 1.2777,
      "step": 7400
    },
    {
      "epoch": 0.4632028035959165,
      "grad_norm": 2.565666675567627,
      "learning_rate": 0.00017086035629258682,
      "loss": 1.3053,
      "step": 7600
    },
    {
      "epoch": 0.47539235105896693,
      "grad_norm": 2.3178117275238037,
      "learning_rate": 0.00017003940563172155,
      "loss": 1.2879,
      "step": 7800
    },
    {
      "epoch": 0.4875818985220174,
      "grad_norm": 2.374337673187256,
      "learning_rate": 0.00016921845497085627,
      "loss": 1.3111,
      "step": 8000
    },
    {
      "epoch": 0.4997714459850678,
      "grad_norm": 2.005439281463623,
      "learning_rate": 0.00016839750430999096,
      "loss": 1.299,
      "step": 8200
    },
    {
      "epoch": 0.5119609934481182,
      "grad_norm": 2.2802734375,
      "learning_rate": 0.00016757655364912571,
      "loss": 1.2682,
      "step": 8400
    },
    {
      "epoch": 0.5241505409111686,
      "grad_norm": 1.315519094467163,
      "learning_rate": 0.0001667556029882604,
      "loss": 1.2392,
      "step": 8600
    },
    {
      "epoch": 0.5363400883742191,
      "grad_norm": 2.1661272048950195,
      "learning_rate": 0.00016593465232739513,
      "loss": 1.2735,
      "step": 8800
    },
    {
      "epoch": 0.5485296358372695,
      "grad_norm": 3.1786978244781494,
      "learning_rate": 0.00016511370166652985,
      "loss": 1.2723,
      "step": 9000
    },
    {
      "epoch": 0.56071918330032,
      "grad_norm": 2.067967414855957,
      "learning_rate": 0.00016429275100566458,
      "loss": 1.2441,
      "step": 9200
    },
    {
      "epoch": 0.5729087307633705,
      "grad_norm": 2.0354273319244385,
      "learning_rate": 0.00016347180034479927,
      "loss": 1.2386,
      "step": 9400
    },
    {
      "epoch": 0.5850982782264208,
      "grad_norm": 3.186629295349121,
      "learning_rate": 0.000162650849683934,
      "loss": 1.2174,
      "step": 9600
    },
    {
      "epoch": 0.5972878256894713,
      "grad_norm": 2.6571967601776123,
      "learning_rate": 0.00016182989902306874,
      "loss": 1.2673,
      "step": 9800
    },
    {
      "epoch": 0.6094773731525217,
      "grad_norm": 2.8846380710601807,
      "learning_rate": 0.00016100894836220344,
      "loss": 1.2394,
      "step": 10000
    },
    {
      "epoch": 0.6216669206155722,
      "grad_norm": 1.3808234930038452,
      "learning_rate": 0.00016018799770133816,
      "loss": 1.2326,
      "step": 10200
    },
    {
      "epoch": 0.6338564680786226,
      "grad_norm": 2.2511212825775146,
      "learning_rate": 0.00015936704704047288,
      "loss": 1.2676,
      "step": 10400
    },
    {
      "epoch": 0.6460460155416731,
      "grad_norm": 3.1782360076904297,
      "learning_rate": 0.0001585460963796076,
      "loss": 1.262,
      "step": 10600
    },
    {
      "epoch": 0.6582355630047234,
      "grad_norm": 4.217194080352783,
      "learning_rate": 0.0001577251457187423,
      "loss": 1.2242,
      "step": 10800
    },
    {
      "epoch": 0.6704251104677739,
      "grad_norm": 2.947582721710205,
      "learning_rate": 0.00015690419505787702,
      "loss": 1.2405,
      "step": 11000
    },
    {
      "epoch": 0.6826146579308243,
      "grad_norm": 3.4339451789855957,
      "learning_rate": 0.00015608324439701175,
      "loss": 1.1945,
      "step": 11200
    },
    {
      "epoch": 0.6948042053938748,
      "grad_norm": 3.500922203063965,
      "learning_rate": 0.00015526229373614647,
      "loss": 1.2786,
      "step": 11400
    },
    {
      "epoch": 0.7069937528569252,
      "grad_norm": 1.9482821226119995,
      "learning_rate": 0.00015444134307528116,
      "loss": 1.2303,
      "step": 11600
    },
    {
      "epoch": 0.7191833003199756,
      "grad_norm": 2.038813829421997,
      "learning_rate": 0.0001536203924144159,
      "loss": 1.2043,
      "step": 11800
    },
    {
      "epoch": 0.731372847783026,
      "grad_norm": 2.253364086151123,
      "learning_rate": 0.0001527994417535506,
      "loss": 1.2411,
      "step": 12000
    },
    {
      "epoch": 0.7435623952460765,
      "grad_norm": 2.5652294158935547,
      "learning_rate": 0.00015197849109268533,
      "loss": 1.226,
      "step": 12200
    },
    {
      "epoch": 0.7557519427091269,
      "grad_norm": 2.455381155014038,
      "learning_rate": 0.00015115754043182005,
      "loss": 1.2398,
      "step": 12400
    },
    {
      "epoch": 0.7679414901721774,
      "grad_norm": 1.7763479948043823,
      "learning_rate": 0.00015033658977095478,
      "loss": 1.1809,
      "step": 12600
    },
    {
      "epoch": 0.7801310376352278,
      "grad_norm": 1.809883713722229,
      "learning_rate": 0.0001495156391100895,
      "loss": 1.159,
      "step": 12800
    },
    {
      "epoch": 0.7923205850982782,
      "grad_norm": 2.754446506500244,
      "learning_rate": 0.0001486946884492242,
      "loss": 1.1738,
      "step": 13000
    },
    {
      "epoch": 0.8045101325613286,
      "grad_norm": 2.560683488845825,
      "learning_rate": 0.00014787373778835894,
      "loss": 1.2037,
      "step": 13200
    },
    {
      "epoch": 0.8166996800243791,
      "grad_norm": 1.5751994848251343,
      "learning_rate": 0.00014705278712749364,
      "loss": 1.1791,
      "step": 13400
    },
    {
      "epoch": 0.8288892274874295,
      "grad_norm": 2.4044172763824463,
      "learning_rate": 0.00014623183646662836,
      "loss": 1.2444,
      "step": 13600
    },
    {
      "epoch": 0.84107877495048,
      "grad_norm": 3.0210812091827393,
      "learning_rate": 0.00014541088580576308,
      "loss": 1.1878,
      "step": 13800
    },
    {
      "epoch": 0.8532683224135303,
      "grad_norm": 2.3149189949035645,
      "learning_rate": 0.0001445899351448978,
      "loss": 1.1958,
      "step": 14000
    },
    {
      "epoch": 0.8654578698765808,
      "grad_norm": 2.836256504058838,
      "learning_rate": 0.0001437689844840325,
      "loss": 1.2235,
      "step": 14200
    },
    {
      "epoch": 0.8776474173396313,
      "grad_norm": 3.2685794830322266,
      "learning_rate": 0.00014294803382316725,
      "loss": 1.2157,
      "step": 14400
    },
    {
      "epoch": 0.8898369648026817,
      "grad_norm": 0.8860700726509094,
      "learning_rate": 0.00014212708316230194,
      "loss": 1.1551,
      "step": 14600
    },
    {
      "epoch": 0.9020265122657322,
      "grad_norm": 1.855765461921692,
      "learning_rate": 0.00014130613250143667,
      "loss": 1.1543,
      "step": 14800
    },
    {
      "epoch": 0.9142160597287826,
      "grad_norm": 2.5619754791259766,
      "learning_rate": 0.0001404851818405714,
      "loss": 1.1893,
      "step": 15000
    },
    {
      "epoch": 0.926405607191833,
      "grad_norm": 2.9082748889923096,
      "learning_rate": 0.0001396642311797061,
      "loss": 1.195,
      "step": 15200
    },
    {
      "epoch": 0.9385951546548834,
      "grad_norm": 2.223703384399414,
      "learning_rate": 0.00013884328051884083,
      "loss": 1.2127,
      "step": 15400
    },
    {
      "epoch": 0.9507847021179339,
      "grad_norm": 2.986544370651245,
      "learning_rate": 0.00013802232985797553,
      "loss": 1.2058,
      "step": 15600
    },
    {
      "epoch": 0.9629742495809843,
      "grad_norm": 2.803737163543701,
      "learning_rate": 0.00013720137919711028,
      "loss": 1.1821,
      "step": 15800
    },
    {
      "epoch": 0.9751637970440348,
      "grad_norm": 1.6623268127441406,
      "learning_rate": 0.00013638042853624497,
      "loss": 1.1697,
      "step": 16000
    },
    {
      "epoch": 0.9873533445070852,
      "grad_norm": 3.2611618041992188,
      "learning_rate": 0.0001355594778753797,
      "loss": 1.1254,
      "step": 16200
    },
    {
      "epoch": 0.9995428919701356,
      "grad_norm": 2.7222514152526855,
      "learning_rate": 0.00013473852721451442,
      "loss": 1.1573,
      "step": 16400
    },
    {
      "epoch": 1.0117019655645285,
      "grad_norm": 2.503680944442749,
      "learning_rate": 0.00013391757655364914,
      "loss": 1.0885,
      "step": 16600
    },
    {
      "epoch": 1.0238915130275787,
      "grad_norm": 3.2513558864593506,
      "learning_rate": 0.00013309662589278384,
      "loss": 1.0942,
      "step": 16800
    },
    {
      "epoch": 1.0360810604906292,
      "grad_norm": 2.2352705001831055,
      "learning_rate": 0.00013227567523191856,
      "loss": 1.0231,
      "step": 17000
    },
    {
      "epoch": 1.0482706079536797,
      "grad_norm": 3.309335708618164,
      "learning_rate": 0.00013145472457105328,
      "loss": 1.0849,
      "step": 17200
    },
    {
      "epoch": 1.06046015541673,
      "grad_norm": 1.974243402481079,
      "learning_rate": 0.000130633773910188,
      "loss": 1.0684,
      "step": 17400
    },
    {
      "epoch": 1.0726497028797806,
      "grad_norm": 2.2723052501678467,
      "learning_rate": 0.0001298128232493227,
      "loss": 1.0766,
      "step": 17600
    },
    {
      "epoch": 1.084839250342831,
      "grad_norm": 2.7455763816833496,
      "learning_rate": 0.00012899187258845745,
      "loss": 1.0684,
      "step": 17800
    },
    {
      "epoch": 1.0970287978058815,
      "grad_norm": 2.5723342895507812,
      "learning_rate": 0.00012817092192759214,
      "loss": 1.0743,
      "step": 18000
    },
    {
      "epoch": 1.109218345268932,
      "grad_norm": 3.167393922805786,
      "learning_rate": 0.00012734997126672687,
      "loss": 1.0565,
      "step": 18200
    },
    {
      "epoch": 1.1214078927319824,
      "grad_norm": 2.317758321762085,
      "learning_rate": 0.00012652902060586162,
      "loss": 1.0446,
      "step": 18400
    },
    {
      "epoch": 1.1335974401950328,
      "grad_norm": 2.9057319164276123,
      "learning_rate": 0.0001257080699449963,
      "loss": 1.0923,
      "step": 18600
    },
    {
      "epoch": 1.1457869876580833,
      "grad_norm": 3.3827972412109375,
      "learning_rate": 0.00012488711928413103,
      "loss": 1.0689,
      "step": 18800
    },
    {
      "epoch": 1.1579765351211337,
      "grad_norm": 2.5839853286743164,
      "learning_rate": 0.00012406616862326576,
      "loss": 1.0135,
      "step": 19000
    },
    {
      "epoch": 1.170166082584184,
      "grad_norm": 3.64978289604187,
      "learning_rate": 0.00012324521796240048,
      "loss": 1.0622,
      "step": 19200
    },
    {
      "epoch": 1.1823556300472344,
      "grad_norm": 2.9216601848602295,
      "learning_rate": 0.00012242426730153517,
      "loss": 1.0991,
      "step": 19400
    },
    {
      "epoch": 1.1945451775102849,
      "grad_norm": 2.681318998336792,
      "learning_rate": 0.00012160331664066991,
      "loss": 1.0178,
      "step": 19600
    },
    {
      "epoch": 1.2067347249733353,
      "grad_norm": 3.292315721511841,
      "learning_rate": 0.00012078236597980462,
      "loss": 1.07,
      "step": 19800
    },
    {
      "epoch": 1.2189242724363858,
      "grad_norm": 2.4455673694610596,
      "learning_rate": 0.00011996141531893934,
      "loss": 1.0818,
      "step": 20000
    },
    {
      "epoch": 1.2311138198994362,
      "grad_norm": 3.6171164512634277,
      "learning_rate": 0.00011914046465807405,
      "loss": 1.0571,
      "step": 20200
    },
    {
      "epoch": 1.2433033673624867,
      "grad_norm": 1.9008901119232178,
      "learning_rate": 0.00011831951399720877,
      "loss": 1.0317,
      "step": 20400
    },
    {
      "epoch": 1.2554929148255372,
      "grad_norm": 2.6374197006225586,
      "learning_rate": 0.00011749856333634348,
      "loss": 1.067,
      "step": 20600
    },
    {
      "epoch": 1.2676824622885876,
      "grad_norm": 1.6671706438064575,
      "learning_rate": 0.0001166776126754782,
      "loss": 1.053,
      "step": 20800
    },
    {
      "epoch": 1.2798720097516378,
      "grad_norm": 2.881953477859497,
      "learning_rate": 0.00011585666201461294,
      "loss": 1.0673,
      "step": 21000
    },
    {
      "epoch": 1.2920615572146885,
      "grad_norm": 3.4389798641204834,
      "learning_rate": 0.00011503571135374765,
      "loss": 1.019,
      "step": 21200
    },
    {
      "epoch": 1.3042511046777387,
      "grad_norm": 2.2173426151275635,
      "learning_rate": 0.00011421476069288237,
      "loss": 1.0554,
      "step": 21400
    },
    {
      "epoch": 1.3164406521407892,
      "grad_norm": 2.312995433807373,
      "learning_rate": 0.00011339381003201708,
      "loss": 1.077,
      "step": 21600
    },
    {
      "epoch": 1.3286301996038397,
      "grad_norm": 2.396860122680664,
      "learning_rate": 0.0001125728593711518,
      "loss": 1.0883,
      "step": 21800
    },
    {
      "epoch": 1.34081974706689,
      "grad_norm": 1.8776346445083618,
      "learning_rate": 0.00011175190871028651,
      "loss": 1.0188,
      "step": 22000
    },
    {
      "epoch": 1.3530092945299406,
      "grad_norm": 4.28844690322876,
      "learning_rate": 0.00011093095804942125,
      "loss": 1.0529,
      "step": 22200
    },
    {
      "epoch": 1.365198841992991,
      "grad_norm": 2.350464105606079,
      "learning_rate": 0.00011011000738855594,
      "loss": 1.0158,
      "step": 22400
    },
    {
      "epoch": 1.3773883894560415,
      "grad_norm": 2.9974586963653564,
      "learning_rate": 0.00010928905672769068,
      "loss": 1.0333,
      "step": 22600
    },
    {
      "epoch": 1.389577936919092,
      "grad_norm": 2.673778772354126,
      "learning_rate": 0.00010846810606682539,
      "loss": 1.044,
      "step": 22800
    },
    {
      "epoch": 1.4017674843821424,
      "grad_norm": 3.2944679260253906,
      "learning_rate": 0.00010764715540596011,
      "loss": 1.0372,
      "step": 23000
    },
    {
      "epoch": 1.4139570318451926,
      "grad_norm": 2.161869525909424,
      "learning_rate": 0.00010682620474509482,
      "loss": 1.0619,
      "step": 23200
    },
    {
      "epoch": 1.4261465793082433,
      "grad_norm": 3.4828929901123047,
      "learning_rate": 0.00010600525408422954,
      "loss": 1.02,
      "step": 23400
    },
    {
      "epoch": 1.4383361267712935,
      "grad_norm": 3.9782700538635254,
      "learning_rate": 0.00010518430342336425,
      "loss": 1.0072,
      "step": 23600
    },
    {
      "epoch": 1.450525674234344,
      "grad_norm": 2.185269594192505,
      "learning_rate": 0.00010436335276249898,
      "loss": 1.1009,
      "step": 23800
    },
    {
      "epoch": 1.4627152216973944,
      "grad_norm": 1.8383991718292236,
      "learning_rate": 0.0001035424021016337,
      "loss": 1.0716,
      "step": 24000
    },
    {
      "epoch": 1.4749047691604449,
      "grad_norm": 3.1413824558258057,
      "learning_rate": 0.00010272145144076842,
      "loss": 0.997,
      "step": 24200
    },
    {
      "epoch": 1.4870943166234953,
      "grad_norm": 1.5870906114578247,
      "learning_rate": 0.00010190050077990314,
      "loss": 1.0335,
      "step": 24400
    },
    {
      "epoch": 1.4992838640865458,
      "grad_norm": 1.8904949426651,
      "learning_rate": 0.00010107955011903785,
      "loss": 1.0691,
      "step": 24600
    },
    {
      "epoch": 1.5114734115495962,
      "grad_norm": 2.8631703853607178,
      "learning_rate": 0.00010025859945817257,
      "loss": 1.0273,
      "step": 24800
    },
    {
      "epoch": 1.5236629590126467,
      "grad_norm": 2.3286304473876953,
      "learning_rate": 9.943764879730728e-05,
      "loss": 1.0785,
      "step": 25000
    },
    {
      "epoch": 1.5358525064756972,
      "grad_norm": 2.4479856491088867,
      "learning_rate": 9.861669813644201e-05,
      "loss": 1.058,
      "step": 25200
    },
    {
      "epoch": 1.5480420539387474,
      "grad_norm": 2.5877935886383057,
      "learning_rate": 9.779574747557672e-05,
      "loss": 1.0286,
      "step": 25400
    },
    {
      "epoch": 1.560231601401798,
      "grad_norm": 3.233041763305664,
      "learning_rate": 9.697479681471144e-05,
      "loss": 1.0608,
      "step": 25600
    },
    {
      "epoch": 1.5724211488648483,
      "grad_norm": 3.0036733150482178,
      "learning_rate": 9.615384615384617e-05,
      "loss": 1.01,
      "step": 25800
    },
    {
      "epoch": 1.584610696327899,
      "grad_norm": 2.3520545959472656,
      "learning_rate": 9.533289549298088e-05,
      "loss": 1.055,
      "step": 26000
    },
    {
      "epoch": 1.5968002437909492,
      "grad_norm": 2.127668857574463,
      "learning_rate": 9.45119448321156e-05,
      "loss": 1.0736,
      "step": 26200
    },
    {
      "epoch": 1.6089897912539997,
      "grad_norm": 2.9214470386505127,
      "learning_rate": 9.369099417125031e-05,
      "loss": 1.045,
      "step": 26400
    },
    {
      "epoch": 1.6211793387170501,
      "grad_norm": 3.185086488723755,
      "learning_rate": 9.287004351038503e-05,
      "loss": 1.0003,
      "step": 26600
    },
    {
      "epoch": 1.6333688861801006,
      "grad_norm": 2.0765678882598877,
      "learning_rate": 9.204909284951975e-05,
      "loss": 1.0123,
      "step": 26800
    },
    {
      "epoch": 1.645558433643151,
      "grad_norm": 3.5601656436920166,
      "learning_rate": 9.122814218865446e-05,
      "loss": 0.9678,
      "step": 27000
    },
    {
      "epoch": 1.6577479811062015,
      "grad_norm": 2.4518959522247314,
      "learning_rate": 9.040719152778918e-05,
      "loss": 1.0229,
      "step": 27200
    },
    {
      "epoch": 1.669937528569252,
      "grad_norm": 3.6508755683898926,
      "learning_rate": 8.958624086692389e-05,
      "loss": 1.0408,
      "step": 27400
    },
    {
      "epoch": 1.6821270760323022,
      "grad_norm": 3.609710454940796,
      "learning_rate": 8.876529020605861e-05,
      "loss": 0.9862,
      "step": 27600
    },
    {
      "epoch": 1.6943166234953528,
      "grad_norm": 2.416271686553955,
      "learning_rate": 8.794433954519334e-05,
      "loss": 0.9562,
      "step": 27800
    },
    {
      "epoch": 1.706506170958403,
      "grad_norm": 4.547244071960449,
      "learning_rate": 8.712338888432806e-05,
      "loss": 0.9895,
      "step": 28000
    },
    {
      "epoch": 1.7186957184214537,
      "grad_norm": 3.484110116958618,
      "learning_rate": 8.630243822346278e-05,
      "loss": 0.981,
      "step": 28200
    },
    {
      "epoch": 1.730885265884504,
      "grad_norm": 3.873579978942871,
      "learning_rate": 8.548148756259749e-05,
      "loss": 0.9828,
      "step": 28400
    },
    {
      "epoch": 1.7430748133475544,
      "grad_norm": 2.797664165496826,
      "learning_rate": 8.466053690173221e-05,
      "loss": 1.0297,
      "step": 28600
    },
    {
      "epoch": 1.7552643608106049,
      "grad_norm": 2.2726123332977295,
      "learning_rate": 8.383958624086694e-05,
      "loss": 0.9651,
      "step": 28800
    },
    {
      "epoch": 1.7674539082736553,
      "grad_norm": 2.5967211723327637,
      "learning_rate": 8.301863558000164e-05,
      "loss": 1.0465,
      "step": 29000
    },
    {
      "epoch": 1.7796434557367058,
      "grad_norm": 3.8825395107269287,
      "learning_rate": 8.219768491913637e-05,
      "loss": 1.0172,
      "step": 29200
    },
    {
      "epoch": 1.7918330031997562,
      "grad_norm": 2.35263991355896,
      "learning_rate": 8.137673425827108e-05,
      "loss": 0.9755,
      "step": 29400
    },
    {
      "epoch": 1.8040225506628067,
      "grad_norm": 2.0102407932281494,
      "learning_rate": 8.05557835974058e-05,
      "loss": 0.9579,
      "step": 29600
    },
    {
      "epoch": 1.816212098125857,
      "grad_norm": 4.09232234954834,
      "learning_rate": 7.973483293654052e-05,
      "loss": 1.0072,
      "step": 29800
    },
    {
      "epoch": 1.8284016455889076,
      "grad_norm": 1.9718297719955444,
      "learning_rate": 7.891388227567523e-05,
      "loss": 1.0017,
      "step": 30000
    },
    {
      "epoch": 1.8405911930519578,
      "grad_norm": 3.134486436843872,
      "learning_rate": 7.809293161480995e-05,
      "loss": 0.9818,
      "step": 30200
    },
    {
      "epoch": 1.8527807405150085,
      "grad_norm": 2.4159507751464844,
      "learning_rate": 7.727198095394467e-05,
      "loss": 1.0136,
      "step": 30400
    },
    {
      "epoch": 1.8649702879780587,
      "grad_norm": 3.0119268894195557,
      "learning_rate": 7.645103029307938e-05,
      "loss": 0.9716,
      "step": 30600
    },
    {
      "epoch": 1.8771598354411092,
      "grad_norm": 5.646399974822998,
      "learning_rate": 7.563007963221412e-05,
      "loss": 0.9677,
      "step": 30800
    },
    {
      "epoch": 1.8893493829041597,
      "grad_norm": 1.9248883724212646,
      "learning_rate": 7.480912897134883e-05,
      "loss": 1.0229,
      "step": 31000
    },
    {
      "epoch": 1.9015389303672101,
      "grad_norm": 2.3100366592407227,
      "learning_rate": 7.398817831048355e-05,
      "loss": 0.9889,
      "step": 31200
    },
    {
      "epoch": 1.9137284778302606,
      "grad_norm": 3.1175942420959473,
      "learning_rate": 7.316722764961826e-05,
      "loss": 1.0004,
      "step": 31400
    },
    {
      "epoch": 1.925918025293311,
      "grad_norm": 2.6180126667022705,
      "learning_rate": 7.234627698875298e-05,
      "loss": 0.9677,
      "step": 31600
    },
    {
      "epoch": 1.9381075727563615,
      "grad_norm": 2.592916250228882,
      "learning_rate": 7.15253263278877e-05,
      "loss": 1.0257,
      "step": 31800
    },
    {
      "epoch": 1.9502971202194117,
      "grad_norm": 2.925882577896118,
      "learning_rate": 7.070437566702241e-05,
      "loss": 1.0097,
      "step": 32000
    },
    {
      "epoch": 1.9624866676824624,
      "grad_norm": 2.3452179431915283,
      "learning_rate": 6.988342500615713e-05,
      "loss": 0.9216,
      "step": 32200
    },
    {
      "epoch": 1.9746762151455126,
      "grad_norm": 2.5807318687438965,
      "learning_rate": 6.906247434529186e-05,
      "loss": 0.9913,
      "step": 32400
    },
    {
      "epoch": 1.9868657626085633,
      "grad_norm": 2.8182952404022217,
      "learning_rate": 6.824152368442657e-05,
      "loss": 0.9906,
      "step": 32600
    },
    {
      "epoch": 1.9990553100716135,
      "grad_norm": 2.5541882514953613,
      "learning_rate": 6.742057302356129e-05,
      "loss": 0.9688,
      "step": 32800
    },
    {
      "epoch": 2.0112143836660064,
      "grad_norm": 2.7279069423675537,
      "learning_rate": 6.6599622362696e-05,
      "loss": 0.9216,
      "step": 33000
    },
    {
      "epoch": 2.023403931129057,
      "grad_norm": 2.9219305515289307,
      "learning_rate": 6.577867170183072e-05,
      "loss": 0.904,
      "step": 33200
    },
    {
      "epoch": 2.0355934785921073,
      "grad_norm": 1.8982046842575073,
      "learning_rate": 6.495772104096544e-05,
      "loss": 0.9012,
      "step": 33400
    },
    {
      "epoch": 2.0477830260551575,
      "grad_norm": 2.1684927940368652,
      "learning_rate": 6.413677038010016e-05,
      "loss": 0.8435,
      "step": 33600
    },
    {
      "epoch": 2.059972573518208,
      "grad_norm": 2.6572515964508057,
      "learning_rate": 6.331581971923489e-05,
      "loss": 0.9177,
      "step": 33800
    },
    {
      "epoch": 2.0721621209812584,
      "grad_norm": 2.075380325317383,
      "learning_rate": 6.24948690583696e-05,
      "loss": 0.883,
      "step": 34000
    },
    {
      "epoch": 2.084351668444309,
      "grad_norm": 3.7676146030426025,
      "learning_rate": 6.167391839750432e-05,
      "loss": 0.912,
      "step": 34200
    },
    {
      "epoch": 2.0965412159073593,
      "grad_norm": 2.862264633178711,
      "learning_rate": 6.085296773663903e-05,
      "loss": 0.9102,
      "step": 34400
    },
    {
      "epoch": 2.10873076337041,
      "grad_norm": 2.2319958209991455,
      "learning_rate": 6.003201707577375e-05,
      "loss": 0.8414,
      "step": 34600
    },
    {
      "epoch": 2.12092031083346,
      "grad_norm": 4.154888153076172,
      "learning_rate": 5.9211066414908464e-05,
      "loss": 0.9231,
      "step": 34800
    },
    {
      "epoch": 2.133109858296511,
      "grad_norm": 2.5215110778808594,
      "learning_rate": 5.8390115754043186e-05,
      "loss": 0.8875,
      "step": 35000
    },
    {
      "epoch": 2.145299405759561,
      "grad_norm": 2.5424411296844482,
      "learning_rate": 5.75691650931779e-05,
      "loss": 0.8741,
      "step": 35200
    },
    {
      "epoch": 2.157488953222612,
      "grad_norm": 3.3107831478118896,
      "learning_rate": 5.674821443231262e-05,
      "loss": 0.9072,
      "step": 35400
    },
    {
      "epoch": 2.169678500685662,
      "grad_norm": 2.6918323040008545,
      "learning_rate": 5.592726377144733e-05,
      "loss": 0.8479,
      "step": 35600
    },
    {
      "epoch": 2.1818680481487123,
      "grad_norm": 2.0859029293060303,
      "learning_rate": 5.5106313110582056e-05,
      "loss": 0.8661,
      "step": 35800
    },
    {
      "epoch": 2.194057595611763,
      "grad_norm": 3.192997455596924,
      "learning_rate": 5.428536244971677e-05,
      "loss": 0.9016,
      "step": 36000
    },
    {
      "epoch": 2.206247143074813,
      "grad_norm": 3.734684467315674,
      "learning_rate": 5.346441178885149e-05,
      "loss": 0.9084,
      "step": 36200
    },
    {
      "epoch": 2.218436690537864,
      "grad_norm": 2.038224697113037,
      "learning_rate": 5.2643461127986216e-05,
      "loss": 0.9061,
      "step": 36400
    },
    {
      "epoch": 2.230626238000914,
      "grad_norm": 2.814666986465454,
      "learning_rate": 5.182251046712093e-05,
      "loss": 0.896,
      "step": 36600
    },
    {
      "epoch": 2.2428157854639648,
      "grad_norm": 3.135396718978882,
      "learning_rate": 5.100155980625565e-05,
      "loss": 0.932,
      "step": 36800
    },
    {
      "epoch": 2.255005332927015,
      "grad_norm": 3.3349666595458984,
      "learning_rate": 5.018060914539037e-05,
      "loss": 0.8701,
      "step": 37000
    },
    {
      "epoch": 2.2671948803900657,
      "grad_norm": 4.240277290344238,
      "learning_rate": 4.9359658484525085e-05,
      "loss": 0.8876,
      "step": 37200
    },
    {
      "epoch": 2.279384427853116,
      "grad_norm": 4.160800457000732,
      "learning_rate": 4.85387078236598e-05,
      "loss": 0.8965,
      "step": 37400
    },
    {
      "epoch": 2.2915739753161666,
      "grad_norm": 3.260765314102173,
      "learning_rate": 4.7717757162794516e-05,
      "loss": 0.8572,
      "step": 37600
    },
    {
      "epoch": 2.303763522779217,
      "grad_norm": 4.056058406829834,
      "learning_rate": 4.689680650192924e-05,
      "loss": 0.8628,
      "step": 37800
    },
    {
      "epoch": 2.3159530702422675,
      "grad_norm": 5.170449256896973,
      "learning_rate": 4.6075855841063954e-05,
      "loss": 0.8685,
      "step": 38000
    },
    {
      "epoch": 2.3281426177053177,
      "grad_norm": 3.5428061485290527,
      "learning_rate": 4.525490518019867e-05,
      "loss": 0.8563,
      "step": 38200
    },
    {
      "epoch": 2.340332165168368,
      "grad_norm": 2.2153804302215576,
      "learning_rate": 4.443395451933339e-05,
      "loss": 0.9027,
      "step": 38400
    },
    {
      "epoch": 2.3525217126314186,
      "grad_norm": 6.533862590789795,
      "learning_rate": 4.361300385846811e-05,
      "loss": 0.9221,
      "step": 38600
    },
    {
      "epoch": 2.364711260094469,
      "grad_norm": 2.5562472343444824,
      "learning_rate": 4.279205319760283e-05,
      "loss": 0.9249,
      "step": 38800
    },
    {
      "epoch": 2.3769008075575195,
      "grad_norm": 5.102926731109619,
      "learning_rate": 4.1971102536737546e-05,
      "loss": 0.9059,
      "step": 39000
    },
    {
      "epoch": 2.3890903550205698,
      "grad_norm": 3.6121108531951904,
      "learning_rate": 4.115015187587226e-05,
      "loss": 0.9176,
      "step": 39200
    },
    {
      "epoch": 2.4012799024836204,
      "grad_norm": 2.3677170276641846,
      "learning_rate": 4.032920121500698e-05,
      "loss": 0.8948,
      "step": 39400
    },
    {
      "epoch": 2.4134694499466707,
      "grad_norm": 2.8649351596832275,
      "learning_rate": 3.950825055414169e-05,
      "loss": 0.8613,
      "step": 39600
    },
    {
      "epoch": 2.4256589974097214,
      "grad_norm": 3.326646566390991,
      "learning_rate": 3.868729989327642e-05,
      "loss": 0.9414,
      "step": 39800
    },
    {
      "epoch": 2.4378485448727716,
      "grad_norm": 2.2417848110198975,
      "learning_rate": 3.786634923241114e-05,
      "loss": 0.9093,
      "step": 40000
    },
    {
      "epoch": 2.450038092335822,
      "grad_norm": 2.8344016075134277,
      "learning_rate": 3.704539857154585e-05,
      "loss": 0.8875,
      "step": 40200
    },
    {
      "epoch": 2.4622276397988725,
      "grad_norm": 3.2543938159942627,
      "learning_rate": 3.622444791068057e-05,
      "loss": 0.8786,
      "step": 40400
    },
    {
      "epoch": 2.4744171872619227,
      "grad_norm": 3.010791063308716,
      "learning_rate": 3.5403497249815284e-05,
      "loss": 0.8982,
      "step": 40600
    },
    {
      "epoch": 2.4866067347249734,
      "grad_norm": 4.1912841796875,
      "learning_rate": 3.4582546588950006e-05,
      "loss": 0.8895,
      "step": 40800
    },
    {
      "epoch": 2.4987962821880236,
      "grad_norm": 2.682018280029297,
      "learning_rate": 3.376159592808472e-05,
      "loss": 0.9345,
      "step": 41000
    },
    {
      "epoch": 2.5109858296510743,
      "grad_norm": 2.777693510055542,
      "learning_rate": 3.2940645267219444e-05,
      "loss": 0.8538,
      "step": 41200
    },
    {
      "epoch": 2.5231753771141245,
      "grad_norm": 3.3576788902282715,
      "learning_rate": 3.211969460635416e-05,
      "loss": 0.8865,
      "step": 41400
    },
    {
      "epoch": 2.535364924577175,
      "grad_norm": 2.031611680984497,
      "learning_rate": 3.1298743945488876e-05,
      "loss": 0.9211,
      "step": 41600
    },
    {
      "epoch": 2.5475544720402254,
      "grad_norm": 2.6660709381103516,
      "learning_rate": 3.0477793284623595e-05,
      "loss": 0.9191,
      "step": 41800
    },
    {
      "epoch": 2.5597440195032757,
      "grad_norm": 2.579759120941162,
      "learning_rate": 2.9656842623758314e-05,
      "loss": 0.901,
      "step": 42000
    },
    {
      "epoch": 2.5719335669663264,
      "grad_norm": 3.260683536529541,
      "learning_rate": 2.883589196289303e-05,
      "loss": 0.8629,
      "step": 42200
    },
    {
      "epoch": 2.584123114429377,
      "grad_norm": 2.7603561878204346,
      "learning_rate": 2.8014941302027748e-05,
      "loss": 0.8842,
      "step": 42400
    },
    {
      "epoch": 2.5963126618924273,
      "grad_norm": 4.5066752433776855,
      "learning_rate": 2.719399064116247e-05,
      "loss": 0.8985,
      "step": 42600
    },
    {
      "epoch": 2.6085022093554775,
      "grad_norm": 4.675235271453857,
      "learning_rate": 2.6373039980297186e-05,
      "loss": 0.8233,
      "step": 42800
    },
    {
      "epoch": 2.620691756818528,
      "grad_norm": 1.9852383136749268,
      "learning_rate": 2.5552089319431905e-05,
      "loss": 0.8382,
      "step": 43000
    },
    {
      "epoch": 2.6328813042815784,
      "grad_norm": 3.9565160274505615,
      "learning_rate": 2.473113865856662e-05,
      "loss": 0.9105,
      "step": 43200
    },
    {
      "epoch": 2.645070851744629,
      "grad_norm": 3.1092071533203125,
      "learning_rate": 2.391018799770134e-05,
      "loss": 0.9128,
      "step": 43400
    },
    {
      "epoch": 2.6572603992076793,
      "grad_norm": 4.240101337432861,
      "learning_rate": 2.3089237336836055e-05,
      "loss": 0.8325,
      "step": 43600
    },
    {
      "epoch": 2.66944994667073,
      "grad_norm": 2.7284603118896484,
      "learning_rate": 2.2268286675970774e-05,
      "loss": 0.8542,
      "step": 43800
    },
    {
      "epoch": 2.68163949413378,
      "grad_norm": 2.021538257598877,
      "learning_rate": 2.1447336015105493e-05,
      "loss": 0.8722,
      "step": 44000
    },
    {
      "epoch": 2.693829041596831,
      "grad_norm": 1.987291693687439,
      "learning_rate": 2.062638535424021e-05,
      "loss": 0.8574,
      "step": 44200
    },
    {
      "epoch": 2.706018589059881,
      "grad_norm": 2.161405086517334,
      "learning_rate": 1.980543469337493e-05,
      "loss": 0.904,
      "step": 44400
    },
    {
      "epoch": 2.7182081365229314,
      "grad_norm": 2.3691773414611816,
      "learning_rate": 1.8984484032509647e-05,
      "loss": 0.8769,
      "step": 44600
    },
    {
      "epoch": 2.730397683985982,
      "grad_norm": 3.433109998703003,
      "learning_rate": 1.8163533371644366e-05,
      "loss": 0.9047,
      "step": 44800
    },
    {
      "epoch": 2.7425872314490327,
      "grad_norm": 2.6300737857818604,
      "learning_rate": 1.734258271077908e-05,
      "loss": 0.8725,
      "step": 45000
    },
    {
      "epoch": 2.754776778912083,
      "grad_norm": 2.804126501083374,
      "learning_rate": 1.65216320499138e-05,
      "loss": 0.8729,
      "step": 45200
    },
    {
      "epoch": 2.766966326375133,
      "grad_norm": 3.1167335510253906,
      "learning_rate": 1.570068138904852e-05,
      "loss": 0.8498,
      "step": 45400
    },
    {
      "epoch": 2.779155873838184,
      "grad_norm": 3.68096923828125,
      "learning_rate": 1.4879730728183237e-05,
      "loss": 0.8264,
      "step": 45600
    },
    {
      "epoch": 2.791345421301234,
      "grad_norm": 3.7013893127441406,
      "learning_rate": 1.4058780067317956e-05,
      "loss": 0.8797,
      "step": 45800
    },
    {
      "epoch": 2.8035349687642848,
      "grad_norm": 2.4811604022979736,
      "learning_rate": 1.3237829406452673e-05,
      "loss": 0.8729,
      "step": 46000
    },
    {
      "epoch": 2.815724516227335,
      "grad_norm": 2.717892646789551,
      "learning_rate": 1.241687874558739e-05,
      "loss": 0.9105,
      "step": 46200
    },
    {
      "epoch": 2.8279140636903852,
      "grad_norm": 3.875293493270874,
      "learning_rate": 1.159592808472211e-05,
      "loss": 0.8745,
      "step": 46400
    },
    {
      "epoch": 2.840103611153436,
      "grad_norm": 2.906745672225952,
      "learning_rate": 1.0774977423856828e-05,
      "loss": 0.829,
      "step": 46600
    },
    {
      "epoch": 2.8522931586164866,
      "grad_norm": 3.904270648956299,
      "learning_rate": 9.954026762991544e-06,
      "loss": 0.8961,
      "step": 46800
    },
    {
      "epoch": 2.864482706079537,
      "grad_norm": 3.647252082824707,
      "learning_rate": 9.133076102126263e-06,
      "loss": 0.8928,
      "step": 47000
    },
    {
      "epoch": 2.876672253542587,
      "grad_norm": 2.5609278678894043,
      "learning_rate": 8.31212544126098e-06,
      "loss": 0.8936,
      "step": 47200
    },
    {
      "epoch": 2.8888618010056377,
      "grad_norm": 1.7131949663162231,
      "learning_rate": 7.491174780395699e-06,
      "loss": 0.8551,
      "step": 47400
    },
    {
      "epoch": 2.901051348468688,
      "grad_norm": 2.44706130027771,
      "learning_rate": 6.670224119530416e-06,
      "loss": 0.8203,
      "step": 47600
    },
    {
      "epoch": 2.9132408959317386,
      "grad_norm": 3.4430079460144043,
      "learning_rate": 5.8492734586651344e-06,
      "loss": 0.8701,
      "step": 47800
    },
    {
      "epoch": 2.925430443394789,
      "grad_norm": 2.171736478805542,
      "learning_rate": 5.0283227977998526e-06,
      "loss": 0.892,
      "step": 48000
    },
    {
      "epoch": 2.9376199908578395,
      "grad_norm": 2.411752223968506,
      "learning_rate": 4.207372136934571e-06,
      "loss": 0.8344,
      "step": 48200
    },
    {
      "epoch": 2.9498095383208898,
      "grad_norm": 4.679902076721191,
      "learning_rate": 3.386421476069288e-06,
      "loss": 0.8844,
      "step": 48400
    },
    {
      "epoch": 2.9619990857839404,
      "grad_norm": 3.218744993209839,
      "learning_rate": 2.5654708152040066e-06,
      "loss": 0.8587,
      "step": 48600
    },
    {
      "epoch": 2.9741886332469907,
      "grad_norm": 2.7703700065612793,
      "learning_rate": 1.7445201543387243e-06,
      "loss": 0.8726,
      "step": 48800
    },
    {
      "epoch": 2.986378180710041,
      "grad_norm": 3.336406707763672,
      "learning_rate": 9.235694934734423e-07,
      "loss": 0.8672,
      "step": 49000
    },
    {
      "epoch": 2.9985677281730916,
      "grad_norm": 0.9892103672027588,
      "learning_rate": 1.0261883260816025e-07,
      "loss": 0.8585,
      "step": 49200
    }
  ],
  "logging_steps": 200,
  "max_steps": 49224,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.406437630017536e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
